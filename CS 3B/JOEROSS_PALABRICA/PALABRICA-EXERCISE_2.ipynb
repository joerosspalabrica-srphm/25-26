{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f307871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\tChoose either one of the following tasks (the output of this task will be used on the next number):\n",
    "#a.\tDevelop a Class in Python called Dense_Layer (included in the submitted notebook).\n",
    "#b.\tCreate a Helper file called neural_network_helper (separate file from the notebook). \n",
    "\n",
    "#The chosen task should have the following functions:\n",
    "#a)\t(10 points) A function to setup/accept the inputs and weights\n",
    "#b)\t(10 points) A function to perform the weighted sum + bias\n",
    "#c)\t(15 points) A function to perform the selected activation function\n",
    "#d)\t(15 points) A function to calculate the loss (predicted output vs target output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b69428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.\tDevelop a Class in Python called Dense_Layer (included in the submitted notebook).\n",
    "import numpy as np\n",
    "\n",
    "class Dense_Layer:\n",
    "    def __init__(self, inputs, weights, bias, activation=None, name=\"\"):\n",
    "        self.inputs = np.array(inputs)\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "        self.z = None\n",
    "        self.output = None\n",
    "\n",
    "#a)\t(10 points) A function to setup/accept the inputs and weights  \n",
    "    def weighted_sum(self):\n",
    "        return np.dot(self.inputs, self.weights) + self.bias\n",
    "\n",
    "#b)\t(10 points) A function to perform the weighted sum + bias\n",
    "    def activate(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == \"softmax\":\n",
    "            exp_vals = np.exp(z - np.max(z))\n",
    "            return exp_vals / np.sum(exp_vals)\n",
    "        else:\n",
    "            return z\n",
    "        \n",
    "#c)\t(15 points) A function to perform the selected activation function\n",
    "    def forward(self, verbose=True):\n",
    "        self.z = self.weighted_sum()\n",
    "        self.output = self.activate(self.z)\n",
    "        if verbose:\n",
    "            print(f\"--- {self.name} ---\")\n",
    "            print(\"Weighted sum (z):\", np.round(self.z, 6))\n",
    "            print(\"Activated output:\", np.round(self.output, 6))\n",
    "            print()\n",
    "        return self.output\n",
    "    \n",
    "#d)\t(15 points) A function to calculate the loss (predicted output vs target output)\n",
    "def cross_entropy_loss(predicted, target):\n",
    "    predicted = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "    return -np.sum(target * np.log(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13523efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Helper functions for mathematical operations\n",
    "def dot_product(vec1, vec2):\n",
    "    \"\"\"Calculate dot product of two vectors\"\"\"\n",
    "    return sum(a * b for a, b in zip(vec1, vec2))\n",
    "\n",
    "def matrix_vector_multiply(matrix, vector):\n",
    "    \"\"\"Multiply matrix by vector\"\"\"\n",
    "    return [dot_product(row, vector) for row in matrix]\n",
    "\n",
    "def vector_add(vec1, vec2):\n",
    "    \"\"\"Add two vectors\"\"\"\n",
    "    return [a + b for a, b in zip(vec1, vec2)]\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [max(0, val) for val in x]\n",
    "    return max(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return [1 / (1 + math.exp(-val)) for val in x]\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def binary_cross_entropy_loss(predicted, target):\n",
    "    \"\"\"Binary cross-entropy loss\"\"\"\n",
    "    predicted = max(min(predicted, 1 - 1e-15), 1e-15)  # Clip values\n",
    "    return -(target * math.log(predicted) + (1 - target) * math.log(1 - predicted))\n",
    "\n",
    "def mse_loss(predicted, target):\n",
    "    \"\"\"Mean squared error loss\"\"\"\n",
    "    return (predicted - target) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78f91da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# QUESTION 2B: BREAST CANCER CLASSIFICATION\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f46a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Set B: Breast Cancer Dataset Classification\n",
    "# Given the following inputs from the Breast Cancer Dataset, using three features: \n",
    "# Mean Radius, Mean Texture, and Mean Smoothness, determine whether the tumor is \n",
    "# Benign (0) or Malignant (1)\n",
    "\n",
    "# Given inputs and target from the Breast Cancer Dataset [cite: 28]\n",
    "X_prob2 = np.array([14.1, 20.3, 0.095])\n",
    "target_prob2 = np.array([1])\n",
    "\n",
    "# Layer 1 (Hidden) parameters with ReLU activation [cite: 29]\n",
    "W1_prob2 = np.array([\n",
    "    [ 0.5, -0.3, 0.8],\n",
    "    [ 0.2, 0.4, -0.6],\n",
    "    [-0.7, 0.9, 0.1]\n",
    "])\n",
    "B1_prob2 = np.array([0.3, -0.5, 0.6])\n",
    "\n",
    "# Layer 2 (Hidden) parameters with Sigmoid activation [cite: 31]\n",
    "W2_prob2 = np.array([\n",
    "    [ 0.6, -0.3],\n",
    "    [ -0.2, 0.5],\n",
    "    [ 0.5, 0.7]\n",
    "])\n",
    "B2_prob2 = np.array([0.1, -0.8])\n",
    "\n",
    "# Layer 3 (Output) parameters with Sigmoid activation [cite: 33]\n",
    "W3_prob2 = np.array([\n",
    "    [0.7],\n",
    "    [-0.5]\n",
    "])\n",
    "B3_prob2 = np.array([0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f0e9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Problem 2: Hidden Layer 1 (ReLU) ---\n",
      "Weighted sum (z): [11.3435  3.4755 -0.2905]\n",
      "Activated output: [11.3435  3.4755  0.    ]\n",
      "\n",
      "--- Problem 2: Hidden Layer 2 (Sigmoid) ---\n",
      "Weighted sum (z): [ 6.211  -2.4653]\n",
      "Activated output: [0.997997 0.078327]\n",
      "\n",
      "--- Problem 2: Output Layer (Sigmoid) ---\n",
      "Weighted sum (z): [0.859434]\n",
      "Activated output: [0.702542]\n",
      "\n",
      "--- Required Outputs for Problem 2 ---\n",
      "Hidden Layer 2 (Output): [0.997997 0.078327]\n",
      "\n",
      "Final predicted probability (Malignant): [0.702542]\n",
      "Predicted class: Malignant (1)\n",
      "\n",
      "Loss (Binary Cross-Entropy): 0.353049\n",
      "Loss (MSE): 0.088481\n"
     ]
    }
   ],
   "source": [
    "# --- Forward Pass for Breast Cancer Problem ---\n",
    "\n",
    "# First Hidden Layer (ReLU) [cite: 29]\n",
    "layer1_prob2 = Dense_Layer(X_prob2, W1_prob2, B1_prob2, activation=\"relu\", name=\"Problem 2: Hidden Layer 1 (ReLU)\")\n",
    "out1_prob2 = layer1_prob2.forward()\n",
    "\n",
    "# Second Hidden Layer (Sigmoid) [cite: 31]\n",
    "layer2_prob2 = Dense_Layer(out1_prob2, W2_prob2, B2_prob2, activation=\"sigmoid\", name=\"Problem 2: Hidden Layer 2 (Sigmoid)\")\n",
    "out2_prob2 = layer2_prob2.forward()\n",
    "\n",
    "# Output Layer (Sigmoid) [cite: 33]\n",
    "output_layer_prob2 = Dense_Layer(out2_prob2, W3_prob2, B3_prob2, activation=\"sigmoid\", name=\"Problem 2: Output Layer (Sigmoid)\")\n",
    "final_output_prob2 = output_layer_prob2.forward()\n",
    "\n",
    "# --- Required Outputs ---\n",
    "print(\"--- Required Outputs for Problem 2 ---\")\n",
    "# Print the output of the second hidden layer as requested [cite: 34]\n",
    "print(\"Hidden Layer 2 (Output):\", np.round(out2_prob2, 6))\n",
    "print()\n",
    "print(\"Final predicted probability (Malignant):\", np.round(final_output_prob2, 6))\n",
    "\n",
    "# Determine the predicted class based on the final output probability\n",
    "pred_class_prob2 = \"Malignant (1)\" if final_output_prob2[0] > 0.5 else \"Benign (0)\"\n",
    "print(\"Predicted class:\", pred_class_prob2)\n",
    "print()\n",
    "\n",
    "# --- Loss Calculation ---\n",
    "# The document asks for \"Loss:\"[cite: 35]. For binary classification,\n",
    "# Binary Cross-Entropy (BCE) is standard, but Mean Squared Error (MSE) is also a valid loss metric.\n",
    "\n",
    "# Binary Cross-Entropy Loss\n",
    "def binary_cross_entropy(predicted, target):\n",
    "    predicted = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(target * np.log(predicted) + (1 - target) * np.log(1 - predicted))\n",
    "\n",
    "bce_loss_prob2 = binary_cross_entropy(final_output_prob2, target_prob2)\n",
    "print(\"Loss (Binary Cross-Entropy):\", np.round(bce_loss_prob2, 6))\n",
    "\n",
    "# Mean Squared Error (MSE) Loss\n",
    "mse_loss_prob2 = np.mean((final_output_prob2 - target_prob2)**2)\n",
    "print(\"Loss (MSE):\", np.round(mse_loss_prob2, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
